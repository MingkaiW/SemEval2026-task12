# SemEval 2026 Task 12 使用指南

## 目录

1. [环境配置](#1-环境配置)
2. [数据集准备](#2-数据集准备)
3. [Baseline 1: LLM API 调用](#3-baseline-1-llm-api-调用)
4. [Baseline 2: UnifiedQA / RoBERTa](#4-baseline-2-unifiedqa--roberta)
5. [Baseline 3: 知识图谱增强](#5-baseline-3-知识图谱增强)
6. [多模态数据处理](#6-多模态数据处理)
7. [常见问题](#7-常见问题)

---

## 1. 环境配置

### 1.1 基础环境

```bash
# 激活已有环境
conda activate py310

# 或创建新环境
# conda create -n py310 python=3.10
# conda activate py310

# 克隆仓库
git clone https://github.com/sooo66/semeval2026-task12-dataset.git
cd semeval2026-task12-dataset
```

### 1.2 依赖安装

```bash
# Baseline 1 & 2 依赖
pip install openai anthropic transformers torch tqdm

# Baseline 3 额外依赖 (知识图谱)
pip install numpy scipy

# 多模态处理依赖
pip install Pillow requests
```

### 1.3 API密钥配置

```bash
# OpenAI
export OPENAI_API_KEY="sk-xxx"

# Anthropic
export ANTHROPIC_API_KEY="sk-ant-xxx"
```

---

## 2. 数据集准备

### 2.1 数据结构

```
semeval2026-task12-dataset/
├── sample_data/          # 示例集 (200 样本, 10 topics)
│   ├── questions.jsonl
│   ├── docs.json
│   └── docs_updated.json
├── train_data/           # 训练集 (1,819 样本, 36 topics)
│   ├── questions.jsonl   # 问题文件
│   ├── docs.json         # 文档文件
│   └── docs_updated.json # 包含本地图片路径
├── dev_data/             # 验证集 (400 样本, 36 topics)
│   ├── questions.jsonl
│   ├── docs.json
│   └── docs_updated.json
├── test_data/            # 测试集 (612 样本) [NEW]
│   ├── questions.jsonl
│   └── docs.json
└── downloaded_images/    # 下载的图片
    ├── sample_data/topic_[1-10]/
    ├── train_data/topic_[1-36]/
    ├── dev_data/topic_[1-36]/
    └── test_data/topic_[1-20]/
```

### 2.2 数据格式

**questions.jsonl** (每行一个JSON对象):
```json
{
  "topic_id": 22,
  "id": "uuid-xxx",
  "target_event": "伊朗对美军基地发动导弹袭击...",
  "option_A": "12月29日，美军对伊拉克进行空袭...",
  "option_B": "2006年后，穆汉迪斯创立了...",
  "option_C": "12月27日，真主党旅袭击了...",
  "option_D": "美国无人机袭击杀死了伊朗将军...",
  "golden_answer": "D"
}
```

**docs.json**:
```json
[
  {
    "topic_id": 22,
    "topic": "伊朗导弹袭击事件",
    "docs": [
      {
        "title": "新闻标题",
        "content": "文章内容...",
        "source": "新闻来源",
        "imageUrl": "base64或URL",
        "id": "d-1"
      }
    ]
  }
]
```

**注意**: 新版数据集使用 `id` 键代替原来的 `uuid`。

### 2.3 图片提取 (可选)

如果需要处理多模态数据：

```bash
# 从docs.json提取图片到本地
python process_images.py
```

这会创建 `downloaded_images/` 目录和 `docs_updated.json` 文件。

---

## 3. Baseline 1: LLM API 调用

### 3.1 概述

直接调用大语言模型API进行因果推理，无需微调。

**支持的模型:**
| 类型 | 模型示例 |
|------|----------|
| OpenAI | gpt-4o, gpt-4o-mini, gpt-4-turbo |
| Anthropic | claude-3-5-sonnet-20241022 |
| HuggingFace | meta-llama/Llama-3.1-8B-Instruct |
| Ollama | llama3.1:8b, qwen2:7b (本地) |
| vLLM | 高性能推理服务器 |

### 3.2 运行命令

```bash
cd single_modality/baseline

# === OpenAI 模型 ===
# 使用 GPT-4o-mini (推荐，性价比高)
python run_baseline.py \
    --model-type openai \
    --model-name gpt-4o-mini \
    --data-split dev

# 使用 GPT-4o (更强)
python run_baseline.py \
    --model-type openai \
    --model-name gpt-4o \
    --data-split dev \
    --output results_gpt4o.json

# === Anthropic 模型 ===
python run_baseline.py \
    --model-type anthropic \
    --model-name claude-3-5-sonnet-20241022 \
    --data-split dev

# === 本地 Ollama 模型 ===
# 先启动 Ollama 服务
ollama serve &
ollama pull llama3.1:8b

python run_baseline.py \
    --model-type ollama \
    --model-name llama3.1:8b \
    --data-split dev

# === 不使用上下文文档 ===
python run_baseline.py \
    --model-type openai \
    --model-name gpt-4o-mini \
    --no-context

# === 快速测试 (限制样本数) ===
python run_baseline.py \
    --model-type openai \
    --model-name gpt-4o-mini \
    --max-samples 20 \
    --output test_results.json
```

### 3.3 参数说明

| 参数 | 说明 | 默认值 |
|------|------|--------|
| `--model-type` | 模型类型 | openai |
| `--model-name` | 具体模型名称 | gpt-4o-mini |
| `--data-split` | 数据集划分 (train/dev/test) | dev |
| `--no-context` | 不使用上下文文档 | False |
| `--max-samples` | 最大样本数 (测试用) | None |
| `--output` | 结果保存路径 | None |

### 3.4 输出结果

```
============================================================
评估结果
============================================================
官方分数 (Score): 0.6250
完全匹配率: 0.5500 (220/400)
部分匹配率: 0.1500 (60/400)
错误率: 0.3000 (120/400)
```

---

## 4. Baseline 2: UnifiedQA / RoBERTa

### 4.1 概述

基于经典NLP研究的baseline实现：
- **UnifiedQA**: T5模型，零样本生成式问答
- **RoBERTa/DeBERTa**: 判别式多选问答，需要微调

### 4.2 数据预处理

```bash
cd single_modality/baseline2

# 预处理数据 (生成UnifiedQA和RoBERTa格式)
python run_baseline2.py preprocess \
    --dataset-dir ../../ \
    --output-dir ./processed_data

# 不包含上下文文档
python run_baseline2.py preprocess \
    --dataset-dir ../../ \
    --output-dir ./processed_data_no_context \
    --no-context
```

预处理后的目录结构：
```
processed_data/
├── train/
│   ├── unifiedqa.jsonl
│   └── roberta_mcqa.jsonl
└── dev/
    ├── unifiedqa.jsonl
    └── roberta_mcqa.jsonl
```

### 4.3 运行 UnifiedQA (零样本)

```bash
# === 基础版本 ===
python run_baseline2.py unifiedqa \
    --data-path ./processed_data/dev/unifiedqa.jsonl \
    --model-name allenai/unifiedqa-t5-base

# === 更强版本 (UnifiedQA-v2) ===
python run_baseline2.py unifiedqa \
    --data-path ./processed_data/dev/unifiedqa.jsonl \
    --model-name allenai/unifiedqa-v2-t5-large-1363200

# === 保存结果 ===
python run_baseline2.py unifiedqa \
    --data-path ./processed_data/dev/unifiedqa.jsonl \
    --model-name allenai/unifiedqa-v2-t5-base-1363200 \
    --output unifiedqa_results.json \
    --batch-size 8
```

**可用模型:**
| 模型 | 参数量 | 说明 |
|------|--------|------|
| `allenai/unifiedqa-t5-small` | 60M | 最快 |
| `allenai/unifiedqa-t5-base` | 220M | 平衡 |
| `allenai/unifiedqa-t5-large` | 770M | 更准确 |
| `allenai/unifiedqa-v2-t5-base-1363200` | 220M | **推荐** |
| `allenai/unifiedqa-v2-t5-large-1363200` | 770M | 最佳零样本 |

### 4.4 运行 RoBERTa (需微调)

```bash
# === 训练模型 ===
python run_baseline2.py roberta \
    --mode train \
    --train-data ./processed_data/train/roberta_mcqa.jsonl \
    --dev-data ./processed_data/dev/roberta_mcqa.jsonl \
    --model-name roberta-base \
    --output-dir ./roberta_output \
    --epochs 3 \
    --batch-size 4

# === 使用 DeBERTa (推荐) ===
python run_baseline2.py roberta \
    --mode train \
    --train-data ./processed_data/train/roberta_mcqa.jsonl \
    --dev-data ./processed_data/dev/roberta_mcqa.jsonl \
    --model-name microsoft/deberta-v3-base \
    --output-dir ./deberta_output \
    --epochs 3 \
    --batch-size 4

# === 预测 ===
python run_baseline2.py roberta \
    --mode predict \
    --data-path ./processed_data/dev/roberta_mcqa.jsonl \
    --model-name ./roberta_output \
    --output roberta_predictions.json
```

**可用模型:**
| 模型 | 参数量 | 说明 |
|------|--------|------|
| `roberta-base` | 125M | 基础版 |
| `roberta-large` | 355M | 更大 |
| `microsoft/deberta-v3-base` | 184M | **推荐微调** |
| `microsoft/deberta-v3-large` | 434M | 最强 |

---

## 5. Baseline 3: 知识图谱增强

### 5.1 概述

将知识图谱嵌入与大语言模型结合，增强因果推理能力。

**核心组件:**
- **KG嵌入模型**: TransE, ComplEx, RotatE
- **知识生成**: COMET-ATOMIC 2020
- **融合方法**: Prompt增强 / 检索增强

### 5.2 构建知识图谱

```bash
cd single_modality/baseline3

# === 简单知识库 (快速) ===
python run_baseline3.py build-kg \
    --data-path ../../train_data \
    --output-dir ./kg_output

# === 使用 COMET 生成常识知识 (需要 GPU ~3GB) ===
python run_baseline3.py build-kg \
    --data-path ../../train_data \
    --output-dir ./kg_output_comet \
    --use-comet

# === 构建 + 训练嵌入 ===
python run_baseline3.py build-kg \
    --data-path ../../train_data \
    --output-dir ./kg_output_full \
    --use-comet \
    --train-embedding \
    --kg-model TransE \
    --embedding-dim 256 \
    --epochs 100 \
    --batch-size 256
```

输出文件:
```
kg_output/
├── knowledge_graph.json  # 知识图谱
├── kg_model.pt           # 嵌入模型 (如果训练)
└── embeddings.npz        # 实体/关系嵌入
```

### 5.3 KG嵌入模型选择

| 模型 | 公式 | 适用场景 |
|------|------|----------|
| **TransE** | `h + r ≈ t` | 快速原型，简单关系 |
| **ComplEx** | 复数Hermitian积 | 非对称关系 |
| **RotatE** | `t = h ◦ r` (旋转) | 复杂关系，最强表达 |

### 5.4 运行 KG 增强 QA

```bash
# === Prompt 增强 (推荐) ===
# 将KG知识添加到LLM提示词中
python run_baseline3.py qa \
    --data-path ../../dev_data \
    --fusion prompt \
    --llm-type openai \
    --llm-model gpt-4o-mini \
    --use-comet

# === 检索增强 ===
# 从KG检索相关三元组
python run_baseline3.py qa \
    --data-path ../../dev_data \
    --fusion retrieval \
    --kg-path ./kg_output_full \
    --llm-model gpt-4o-mini

# === 使用 Anthropic ===
python run_baseline3.py qa \
    --data-path ../../dev_data \
    --fusion prompt \
    --llm-type anthropic \
    --llm-model claude-3-5-sonnet-20241022

# === 保存结果 ===
python run_baseline3.py qa \
    --data-path ../../dev_data \
    --fusion prompt \
    --llm-model gpt-4o-mini \
    --output kg_qa_results.json \
    --max-samples 100
```

### 5.5 超参数建议

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| `embedding_dim` | 256 | 嵌入维度 |
| `epochs` | 100 | 训练轮数 |
| `batch_size` | 256 | 批大小 |
| `margin` | 1.0 (TransE) / 9.0 (RotatE) | 损失边界 |
| `learning_rate` | 0.001 | 学习率 |

---

## 6. 多模态数据处理

### 6.1 图片提取流程

```bash
# 从 docs.json 提取图片
python process_images.py
```

这会：
1. 从 `docs.json` 读取 base64/URL 图片
2. 保存到 `downloaded_images/{split}/topic_{id}/`
3. 创建 `docs_updated.json` (包含 `local_image_path`)

### 6.2 VaLiK 数据准备

```bash
# 1. 准备 VaLiK 格式数据
python preparae_dataset/prepare_semeval_for_valik.py

# 2. 生成图片描述 (需要 Ollama)
ollama pull qwen3-vl:8b
ollama serve &

cd VaLiK/src
python Image_to_Text.py \
    --input ../../valik_prepared/train_data/images \
    qwen3 --qwen3_version 8b

# 3. 合并文本和描述
cd ../..
python merge_texts.py

# 4. 构建知识图谱
cd VaLiK/src/LightRAG
python lightrag_ollama_demo_semeval.py
```

### 6.3 多模态 ML Baseline

```bash
cd baselines

# 快速测试 (仅 Logistic Regression)
./run_quick_test.sh

# 完整评估 (所有分类器)
./run_all_splits.sh

# 使用图片描述
python ml_mlp_baseline.py \
    --train_questions ../train_data/questions.jsonl \
    --train_docs ../train_data/docs_updated.json \
    --dev_questions ../dev_data/questions.jsonl \
    --dev_docs ../dev_data/docs_updated.json \
    --use_image_descriptions \
    --img2txt_model qwen3 \
    --save_predictions
```

---

## 7. 常见问题

### Q1: API 调用报错 "API key not found"

```bash
# 检查环境变量
echo $OPENAI_API_KEY
echo $ANTHROPIC_API_KEY

# 或在代码中直接设置
export OPENAI_API_KEY="sk-xxx"
```

### Q2: CUDA out of memory

```bash
# 减小批大小
--batch-size 2

# 使用更小的模型
--model-name allenai/unifiedqa-t5-small
```

### Q3: Ollama 连接失败

```bash
# 确保 Ollama 正在运行
ollama serve &

# 检查模型是否已下载
ollama list
ollama pull llama3.1:8b
```

### Q4: 预处理后找不到文件

```bash
# 检查输出目录
ls -la single_modality/baseline2/processed_data/

# 重新运行预处理
python run_baseline2.py preprocess --dataset-dir ../../
```

### Q5: 知识图谱构建失败

```bash
# 检查数据路径
ls ../../train_data/questions.jsonl

# 不使用 COMET (避免 GPU 问题)
python run_baseline3.py build-kg \
    --data-path ../../train_data \
    --output-dir ./kg_output
```

---

## 评估指标说明

**官方评分规则:**
- **1.0分**: 完全匹配 (预测 = 标准答案)
- **0.5分**: 部分匹配 (预测是标准答案的真子集)
- **0.0分**: 错误 (空预测、超集、或错误选项)

**示例:**
| 预测 | 标准答案 | 得分 |
|------|----------|------|
| {A} | {A} | 1.0 |
| {A,B} | {A,B} | 1.0 |
| {A} | {A,B} | 0.5 |
| {A,B} | {A} | 0.0 |
| {C} | {A} | 0.0 |

---

## 推荐工作流

### 快速验证
```bash
# 1. LLM API (最快)
cd single_modality/baseline
python run_baseline.py --model-type openai --model-name gpt-4o-mini --max-samples 50

# 2. UnifiedQA 零样本
cd ../baseline2
python run_baseline2.py preprocess --dataset-dir ../../
python run_baseline2.py unifiedqa --data-path ./processed_data/dev/unifiedqa.jsonl
```

### 完整评估
```bash
# 1. 所有 LLM 模型
for model in gpt-4o-mini gpt-4o; do
    python run_baseline.py --model-type openai --model-name $model --output results_$model.json
done

# 2. 微调 DeBERTa
python run_baseline2.py roberta --mode train \
    --train-data ./processed_data/train/roberta_mcqa.jsonl \
    --dev-data ./processed_data/dev/roberta_mcqa.jsonl \
    --model-name microsoft/deberta-v3-base

# 3. KG 增强
cd ../baseline3
python run_baseline3.py build-kg --data-path ../../train_data --use-comet --train-embedding
python run_baseline3.py qa --data-path ../../dev_data --fusion prompt
```

---

## 参考资源

- [官方数据集](https://github.com/sooo66/semeval2026-task12-dataset)
- [竞赛页面 (Codabench)](https://www.codabench.org/competitions/12440/)
- [SemEval 2026](https://semeval.github.io/SemEval2026/)
- [UnifiedQA 论文](https://arxiv.org/abs/2005.00700)
- [COMET-ATOMIC 2020](https://github.com/allenai/comet-atomic-2020)

---

*最后更新: 2025-01-22 (新增 test_data, 数据格式: uuid→id)*
