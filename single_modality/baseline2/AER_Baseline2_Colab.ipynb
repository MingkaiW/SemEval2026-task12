{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SemEval 2026 Task 12: Abductive Event Reasoning\n",
        "## Baseline 2: UnifiedQA + RoBERTa MCQA\n",
        "\n",
        "本Notebook实现两个经典baseline:\n",
        "1. **UnifiedQA** (T5-based) - 生成式，零样本推理\n",
        "2. **RoBERTa for Multiple Choice** - 判别式，需要微调\n",
        "\n",
        "### 参考文献\n",
        "- [UnifiedQA: Crossing Format Boundaries with a Single QA System](https://arxiv.org/abs/2005.00700)\n",
        "- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 环境配置"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装依赖\n",
        "!pip install -q transformers torch sentencepiece accelerate tqdm"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 下载数据集\n",
        "!git clone https://github.com/sooo66/semeval2026-task12-dataset.git\n",
        "DATASET_DIR = \"semeval2026-task12-dataset\""
      ],
      "metadata": {
        "id": "download_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 数据加载与预处理"
      ],
      "metadata": {
        "id": "data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Set\n",
        "\n",
        "@dataclass\n",
        "class AERInstance:\n",
        "    id: str\n",
        "    topic_id: str\n",
        "    target_event: str\n",
        "    options: Dict[str, str]\n",
        "    golden_answer: Optional[List[str]] = None\n",
        "    docs: Optional[List[Dict]] = None\n",
        "\n",
        "\n",
        "def load_aer_data(data_dir: str) -> List[AERInstance]:\n",
        "    \"\"\"加载AER数据集\"\"\"\n",
        "    data_path = Path(data_dir)\n",
        "\n",
        "    # 加载文档\n",
        "    docs_map = {}\n",
        "    with open(data_path / \"docs.json\", 'r', encoding='utf-8') as f:\n",
        "        for item in json.load(f):\n",
        "            docs_map[item['topic_id']] = item\n",
        "\n",
        "    # 加载问题\n",
        "    instances = []\n",
        "    with open(data_path / \"questions.jsonl\", 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            data = json.loads(line)\n",
        "            options = {k: data.get(f\"option_{k}\", \"\") for k in \"ABCD\"}\n",
        "            golden = [a.strip() for a in data.get(\"golden_answer\", \"\").split(\",\")] if data.get(\"golden_answer\") else None\n",
        "            docs = docs_map.get(data[\"topic_id\"], {}).get(\"docs\", [])\n",
        "\n",
        "            instances.append(AERInstance(\n",
        "                id=data[\"id\"],\n",
        "                topic_id=data[\"topic_id\"],\n",
        "                target_event=data[\"target_event\"],\n",
        "                options=options,\n",
        "                golden_answer=golden,\n",
        "                docs=docs\n",
        "            ))\n",
        "\n",
        "    return instances\n",
        "\n",
        "\n",
        "# 加载数据\n",
        "train_data = load_aer_data(f\"{DATASET_DIR}/train_data\")\n",
        "dev_data = load_aer_data(f\"{DATASET_DIR}/dev_data\")\n",
        "\n",
        "print(f\"训练集: {len(train_data)} 样本\")\n",
        "print(f\"开发集: {len(dev_data)} 样本\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_context(instance: AERInstance, max_len: int = 1500) -> str:\n",
        "    \"\"\"准备上下文\"\"\"\n",
        "    if not instance.docs:\n",
        "        return \"\"\n",
        "\n",
        "    parts = []\n",
        "    total = 0\n",
        "    for doc in instance.docs[:5]:\n",
        "        title = doc.get(\"title\", \"\")\n",
        "        content = doc.get(\"content\", doc.get(\"summary\", \"\"))[:400]\n",
        "        text = f\"[{title}] {content}\" if title else content\n",
        "        if total + len(text) > max_len:\n",
        "            break\n",
        "        parts.append(text)\n",
        "        total += len(text)\n",
        "    return \" \".join(parts)\n",
        "\n",
        "\n",
        "def format_for_unifiedqa(instance: AERInstance, include_context: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    格式化为UnifiedQA输入\n",
        "\n",
        "    格式: question \\\\n (A) opt1 (B) opt2 (C) opt3 (D) opt4\n",
        "    带context: context \\\\n question \\\\n options\n",
        "    \"\"\"\n",
        "    question = f\"What is the most plausible cause of: {instance.target_event}\"\n",
        "    options = \" \".join([f\"({k}) {v}\" for k, v in instance.options.items()])\n",
        "\n",
        "    if include_context:\n",
        "        context = prepare_context(instance)\n",
        "        if context:\n",
        "            return f\"{context} \\\\n {question} \\\\n {options}\"\n",
        "\n",
        "    return f\"{question} \\\\n {options}\"\n",
        "\n",
        "\n",
        "def format_for_roberta(instance: AERInstance, include_context: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    格式化为RoBERTa MCQA输入\n",
        "\n",
        "    返回 SWAG-style 格式\n",
        "    \"\"\"\n",
        "    question = f\"What is the most plausible cause of: {instance.target_event}\"\n",
        "\n",
        "    if include_context:\n",
        "        context = prepare_context(instance)\n",
        "        if context:\n",
        "            question = f\"Context: {context}\\n\\nQuestion: {question}\"\n",
        "\n",
        "    label = None\n",
        "    labels_all = None\n",
        "    if instance.golden_answer:\n",
        "        label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
        "        labels_all = [label_map[a] for a in instance.golden_answer]\n",
        "        label = labels_all[0]\n",
        "\n",
        "    return {\n",
        "        \"id\": instance.id,\n",
        "        \"sent1\": question,\n",
        "        \"sent2\": \"\",\n",
        "        \"ending0\": instance.options[\"A\"],\n",
        "        \"ending1\": instance.options[\"B\"],\n",
        "        \"ending2\": instance.options[\"C\"],\n",
        "        \"ending3\": instance.options[\"D\"],\n",
        "        \"label\": label,\n",
        "        \"labels_all\": labels_all\n",
        "    }\n",
        "\n",
        "\n",
        "# 测试格式化\n",
        "sample = train_data[0]\n",
        "print(\"=== UnifiedQA 格式 ===\")\n",
        "print(format_for_unifiedqa(sample, include_context=False)[:500])\n",
        "print(\"\\n=== RoBERTa MCQA 格式 ===\")\n",
        "roberta_sample = format_for_roberta(sample, include_context=False)\n",
        "print(f\"Question: {roberta_sample['sent1'][:200]}...\")\n",
        "print(f\"Options: {roberta_sample['ending0'][:50]}...\")"
      ],
      "metadata": {
        "id": "preprocessing"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 评估函数"
      ],
      "metadata": {
        "id": "eval"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def calculate_score(pred: Set[str], gold: Set[str]) -> float:\n",
        "    if not pred:\n",
        "        return 0.0\n",
        "    if pred == gold:\n",
        "        return 1.0\n",
        "    if pred < gold:  # 真子集\n",
        "        return 0.5\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def parse_prediction(text: str) -> Set[str]:\n",
        "    \"\"\"解析模型输出\"\"\"\n",
        "    result = set()\n",
        "    for opt in \"ABCD\":\n",
        "        if re.search(rf'\\b{opt}\\b', text.upper()):\n",
        "            result.add(opt)\n",
        "    return result\n",
        "\n",
        "\n",
        "def evaluate(preds: List[Set[str]], golds: List[Set[str]]) -> dict:\n",
        "    scores = [calculate_score(p, g) for p, g in zip(preds, golds)]\n",
        "    n = len(scores)\n",
        "    return {\n",
        "        \"score\": sum(scores) / n,\n",
        "        \"exact_match\": sum(1 for s in scores if s == 1.0) / n,\n",
        "        \"partial_match\": sum(1 for s in scores if s == 0.5) / n,\n",
        "        \"wrong\": sum(1 for s in scores if s == 0.0) / n\n",
        "    }"
      ],
      "metadata": {
        "id": "evaluation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. UnifiedQA Baseline (零样本)\n",
        "\n",
        "### 可用模型\n",
        "- `allenai/unifiedqa-t5-small` (~250MB)\n",
        "- `allenai/unifiedqa-t5-base` (~900MB) ✓ 推荐\n",
        "- `allenai/unifiedqa-t5-large` (~3GB)\n",
        "- `allenai/unifiedqa-v2-t5-base-1363200` (v2版本，更强)"
      ],
      "metadata": {
        "id": "unifiedqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from tqdm import tqdm\n",
        "\n",
        "class UnifiedQABaseline:\n",
        "    def __init__(self, model_name=\"allenai/unifiedqa-t5-base\"):\n",
        "        self.model_name = model_name\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        print(f\"加载模型: {model_name}\")\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "        print(f\"模型已加载到 {self.device}\")\n",
        "\n",
        "    def predict(self, inputs: List[str], batch_size: int = 8) -> List[str]:\n",
        "        results = []\n",
        "\n",
        "        for i in tqdm(range(0, len(inputs), batch_size), desc=\"UnifiedQA\"):\n",
        "            batch = inputs[i:i+batch_size]\n",
        "\n",
        "            encoded = self.tokenizer(\n",
        "                batch,\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=512,\n",
        "                truncation=True,\n",
        "                padding=True\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **encoded,\n",
        "                    max_length=32,\n",
        "                    num_beams=4,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            results.extend(decoded)\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "def run_unifiedqa_eval(data: List[AERInstance],\n",
        "                       model_name: str = \"allenai/unifiedqa-t5-base\",\n",
        "                       include_context: bool = True,\n",
        "                       max_samples: int = None):\n",
        "    \"\"\"运行UnifiedQA评估\"\"\"\n",
        "\n",
        "    if max_samples:\n",
        "        data = data[:max_samples]\n",
        "\n",
        "    # 准备输入\n",
        "    inputs = [format_for_unifiedqa(inst, include_context) for inst in data]\n",
        "    goldens = [set(inst.golden_answer) for inst in data if inst.golden_answer]\n",
        "\n",
        "    # 预测\n",
        "    model = UnifiedQABaseline(model_name)\n",
        "    raw_outputs = model.predict(inputs)\n",
        "    predictions = [parse_prediction(out) for out in raw_outputs]\n",
        "\n",
        "    # 评估\n",
        "    results = evaluate(predictions, goldens)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"UnifiedQA Results ({model_name})\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Score: {results['score']:.4f}\")\n",
        "    print(f\"Exact Match: {results['exact_match']:.4f}\")\n",
        "    print(f\"Partial Match: {results['partial_match']:.4f}\")\n",
        "\n",
        "    return results, predictions, raw_outputs"
      ],
      "metadata": {
        "id": "unifiedqa_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 运行UnifiedQA评估 (取消注释运行)\n",
        "\n",
        "# 使用base模型，不含context（更快）\n",
        "# uqa_results, uqa_preds, uqa_raw = run_unifiedqa_eval(\n",
        "#     dev_data,\n",
        "#     model_name=\"allenai/unifiedqa-t5-base\",\n",
        "#     include_context=False,\n",
        "#     max_samples=50  # 先测试50个样本\n",
        "# )\n",
        "\n",
        "# 使用v2模型，含context（更准确）\n",
        "# uqa_results, uqa_preds, uqa_raw = run_unifiedqa_eval(\n",
        "#     dev_data,\n",
        "#     model_name=\"allenai/unifiedqa-v2-t5-base-1363200\",\n",
        "#     include_context=True\n",
        "# )"
      ],
      "metadata": {
        "id": "run_unifiedqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. RoBERTa MCQA Baseline (需要微调)\n",
        "\n",
        "### 可用模型\n",
        "- `roberta-base` (~500MB)\n",
        "- `roberta-large` (~1.4GB)\n",
        "- `microsoft/deberta-v3-base` (~400MB) ✓ 推荐"
      ],
      "metadata": {
        "id": "roberta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
        "\n",
        "\n",
        "class MCQADataset(Dataset):\n",
        "    def __init__(self, data: List[dict], tokenizer, max_length=256):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        question = item[\"sent1\"]\n",
        "        choices = [item[f\"ending{i}\"] for i in range(4)]\n",
        "\n",
        "        encodings = []\n",
        "        for choice in choices:\n",
        "            enc = self.tokenizer(\n",
        "                question, choice,\n",
        "                max_length=self.max_length,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            encodings.append({k: v.squeeze(0) for k, v in enc.items()})\n",
        "\n",
        "        result = {\n",
        "            \"input_ids\": torch.stack([e[\"input_ids\"] for e in encodings]),\n",
        "            \"attention_mask\": torch.stack([e[\"attention_mask\"] for e in encodings]),\n",
        "            \"id\": item[\"id\"]\n",
        "        }\n",
        "\n",
        "        if item.get(\"label\") is not None:\n",
        "            result[\"labels\"] = torch.tensor(item[\"label\"])\n",
        "            result[\"labels_all\"] = item.get(\"labels_all\", [item[\"label\"]])\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class RoBERTaMCQA:\n",
        "    def __init__(self, model_name=\"roberta-base\"):\n",
        "        self.model_name = model_name\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        print(f\"加载模型: {model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForMultipleChoice.from_pretrained(model_name).to(self.device)\n",
        "        print(f\"模型已加载到 {self.device}\")\n",
        "\n",
        "    def train(self, train_data: List[dict], dev_data: List[dict] = None,\n",
        "              epochs=3, batch_size=4, lr=2e-5):\n",
        "        \"\"\"训练模型\"\"\"\n",
        "        from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "        train_dataset = MCQADataset(train_data, self.tokenizer)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
        "        total_steps = len(train_loader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        print(f\"开始训练: {len(train_data)} 样本, {epochs} epochs\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "\n",
        "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=batch[\"input_ids\"].to(self.device),\n",
        "                    attention_mask=batch[\"attention_mask\"].to(self.device),\n",
        "                    labels=batch[\"labels\"].to(self.device)\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "            if dev_data:\n",
        "                dev_results = self.evaluate(dev_data, batch_size)\n",
        "                print(f\"Dev Score: {dev_results['score']:.4f}\")\n",
        "\n",
        "    def evaluate(self, data: List[dict], batch_size=4) -> dict:\n",
        "        \"\"\"评估模型\"\"\"\n",
        "        self.model.eval()\n",
        "        dataset = MCQADataset(data, self.tokenizer)\n",
        "        loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "        predictions = []\n",
        "        goldens = []\n",
        "        label_map = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(loader, desc=\"Evaluating\"):\n",
        "                outputs = self.model(\n",
        "                    input_ids=batch[\"input_ids\"].to(self.device),\n",
        "                    attention_mask=batch[\"attention_mask\"].to(self.device)\n",
        "                )\n",
        "\n",
        "                preds = outputs.logits.argmax(dim=-1).cpu().tolist()\n",
        "\n",
        "                for i, pred in enumerate(preds):\n",
        "                    predictions.append({label_map[pred]})\n",
        "                    if \"labels_all\" in batch:\n",
        "                        labels = batch[\"labels_all\"][i]\n",
        "                        goldens.append({label_map[l] for l in labels})\n",
        "\n",
        "        return evaluate(predictions, goldens)"
      ],
      "metadata": {
        "id": "roberta_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 准备RoBERTa格式数据\n",
        "train_roberta = [format_for_roberta(inst, include_context=True) for inst in train_data]\n",
        "dev_roberta = [format_for_roberta(inst, include_context=True) for inst in dev_data]\n",
        "\n",
        "print(f\"RoBERTa格式 - 训练: {len(train_roberta)}, 开发: {len(dev_roberta)}\")"
      ],
      "metadata": {
        "id": "prepare_roberta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练RoBERTa (取消注释运行)\n",
        "\n",
        "# roberta = RoBERTaMCQA(\"roberta-base\")\n",
        "# roberta.train(\n",
        "#     train_roberta,\n",
        "#     dev_roberta,\n",
        "#     epochs=3,\n",
        "#     batch_size=4\n",
        "# )\n",
        "\n",
        "# 最终评估\n",
        "# final_results = roberta.evaluate(dev_roberta)\n",
        "# print(f\"\\nFinal Dev Score: {final_results['score']:.4f}\")"
      ],
      "metadata": {
        "id": "train_roberta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 零样本对比: 不训练直接预测\n",
        "\n",
        "如果你想快速测试，可以使用RoBERTa的零样本预测（不微调）"
      ],
      "metadata": {
        "id": "zero_shot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 零样本RoBERTa (不推荐，效果较差)\n",
        "# roberta_zs = RoBERTaMCQA(\"roberta-base\")\n",
        "# zs_results = roberta_zs.evaluate(dev_roberta[:50])\n",
        "# print(f\"Zero-shot Score: {zs_results['score']:.4f}\")"
      ],
      "metadata": {
        "id": "zero_shot_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 结果对比"
      ],
      "metadata": {
        "id": "comparison"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 汇总结果 (填入你的实验结果)\n",
        "results_summary = {\n",
        "    \"Random Baseline\": {\"score\": 0.15, \"exact_match\": 0.10, \"partial_match\": 0.10},\n",
        "    # \"UnifiedQA-base\": uqa_results,\n",
        "    # \"UnifiedQA-v2\": uqa_v2_results,\n",
        "    # \"RoBERTa-base (fine-tuned)\": final_results,\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(results_summary).T\n",
        "print(\"\\n=== 结果对比 ===\")\n",
        "print(df.round(4).to_string())"
      ],
      "metadata": {
        "id": "compare"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. 生成提交文件"
      ],
      "metadata": {
        "id": "submission"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_submission(predictions: List[Set[str]], ids: List[str], output_path: str):\n",
        "    \"\"\"生成提交文件\"\"\"\n",
        "    with open(output_path, 'w') as f:\n",
        "        for id_, pred in zip(ids, predictions):\n",
        "            answer = \",\".join(sorted(pred)) if pred else \"A\"\n",
        "            f.write(f\"{id_}\\t{answer}\\n\")\n",
        "    print(f\"提交文件已保存到: {output_path}\")\n",
        "\n",
        "# 生成提交 (取消注释)\n",
        "# test_data = load_aer_data(f\"{DATASET_DIR}/test_data\")\n",
        "# test_inputs = [format_for_unifiedqa(inst) for inst in test_data]\n",
        "# test_outputs = model.predict(test_inputs)\n",
        "# test_preds = [parse_prediction(out) for out in test_outputs]\n",
        "# generate_submission(test_preds, [inst.id for inst in test_data], \"submission.txt\")"
      ],
      "metadata": {
        "id": "submission_code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
