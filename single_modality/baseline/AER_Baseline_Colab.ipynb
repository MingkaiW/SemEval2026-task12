{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SemEval 2026 Task 12: Abductive Event Reasoning\n",
        "## Baseline 实验 Notebook\n",
        "\n",
        "本 Notebook 提供了多种 baseline 方法来运行 AER 任务:\n",
        "1. **OpenAI API** (GPT-4o, GPT-4o-mini)\n",
        "2. **Anthropic API** (Claude-3.5-Sonnet)\n",
        "3. **HuggingFace** (Llama-3.1, Qwen2, etc.)\n",
        "4. **Ollama** (本地部署)"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 环境配置"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装依赖\n",
        "!pip install -q openai anthropic transformers torch tqdm"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 下载数据集\n",
        "!git clone https://github.com/sooo66/semeval2026-task12-dataset.git"
      ],
      "metadata": {
        "id": "download_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 数据加载和探索"
      ],
      "metadata": {
        "id": "data_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Set\n",
        "\n",
        "@dataclass\n",
        "class AERInstance:\n",
        "    \"\"\"单个AER任务实例\"\"\"\n",
        "    id: str\n",
        "    topic_id: str\n",
        "    target_event: str\n",
        "    options: Dict[str, str]\n",
        "    golden_answer: Optional[List[str]] = None\n",
        "    docs: Optional[List[Dict]] = None\n",
        "\n",
        "\n",
        "def load_aer_data(data_dir: str) -> List[AERInstance]:\n",
        "    \"\"\"加载AER数据集\"\"\"\n",
        "    data_path = Path(data_dir)\n",
        "    \n",
        "    # 加载文档\n",
        "    docs_map = {}\n",
        "    with open(data_path / \"docs.json\", 'r', encoding='utf-8') as f:\n",
        "        for item in json.load(f):\n",
        "            docs_map[item['topic_id']] = item\n",
        "    \n",
        "    # 加载问题\n",
        "    instances = []\n",
        "    with open(data_path / \"questions.jsonl\", 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            data = json.loads(line)\n",
        "            \n",
        "            options = {k: data.get(f\"option_{k}\", \"\") for k in \"ABCD\"}\n",
        "            golden = [a.strip() for a in data.get(\"golden_answer\", \"\").split(\",\")] if data.get(\"golden_answer\") else None\n",
        "            docs = docs_map.get(data[\"topic_id\"], {}).get(\"docs\", [])\n",
        "            \n",
        "            instances.append(AERInstance(\n",
        "                id=data[\"id\"],\n",
        "                topic_id=data[\"topic_id\"],\n",
        "                target_event=data[\"target_event\"],\n",
        "                options=options,\n",
        "                golden_answer=golden,\n",
        "                docs=docs\n",
        "            ))\n",
        "    \n",
        "    return instances"
      ],
      "metadata": {
        "id": "data_loader"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载数据\n",
        "train_data = load_aer_data(\"semeval2026-task12-dataset/train_data\")\n",
        "dev_data = load_aer_data(\"semeval2026-task12-dataset/dev_data\")\n",
        "\n",
        "print(f\"训练集: {len(train_data)} 样本\")\n",
        "print(f\"开发集: {len(dev_data)} 样本\")\n",
        "\n",
        "# 查看样例\n",
        "sample = train_data[0]\n",
        "print(f\"\\n=== 样例 ===\")\n",
        "print(f\"ID: {sample.id}\")\n",
        "print(f\"目标事件: {sample.target_event}\")\n",
        "print(f\"\\n选项:\")\n",
        "for opt, text in sample.options.items():\n",
        "    print(f\"  {opt}: {text[:100]}...\" if len(text) > 100 else f\"  {opt}: {text}\")\n",
        "print(f\"\\n正确答案: {sample.golden_answer}\")\n",
        "print(f\"相关文档数: {len(sample.docs) if sample.docs else 0}\")"
      ],
      "metadata": {
        "id": "explore_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 评估函数"
      ],
      "metadata": {
        "id": "eval_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def calculate_score(prediction: Set[str], golden: Set[str]) -> float:\n",
        "    \"\"\"计算单个实例分数\"\"\"\n",
        "    if not prediction:\n",
        "        return 0.0\n",
        "    if prediction == golden:\n",
        "        return 1.0\n",
        "    if prediction < golden:  # 真子集\n",
        "        return 0.5\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def parse_prediction(pred_str: str) -> Set[str]:\n",
        "    \"\"\"解析模型输出\"\"\"\n",
        "    result = set()\n",
        "    for opt in \"ABCD\":\n",
        "        if re.search(rf'\\b{opt}\\b', pred_str.upper()):\n",
        "            result.add(opt)\n",
        "    return result\n",
        "\n",
        "\n",
        "def evaluate(predictions: List[Set[str]], goldens: List[Set[str]]) -> dict:\n",
        "    \"\"\"评估整个数据集\"\"\"\n",
        "    scores = [calculate_score(p, g) for p, g in zip(predictions, goldens)]\n",
        "    n = len(scores)\n",
        "    \n",
        "    exact = sum(1 for s in scores if s == 1.0)\n",
        "    partial = sum(1 for s in scores if s == 0.5)\n",
        "    \n",
        "    return {\n",
        "        \"score\": sum(scores) / n,\n",
        "        \"exact_match\": exact / n,\n",
        "        \"partial_match\": partial / n,\n",
        "        \"wrong\": (n - exact - partial) / n\n",
        "    }"
      ],
      "metadata": {
        "id": "evaluation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prompt 模板"
      ],
      "metadata": {
        "id": "prompt_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(instance: AERInstance, use_context: bool = True, max_context_len: int = 3000) -> tuple:\n",
        "    \"\"\"创建 Prompt\"\"\"\n",
        "    \n",
        "    system_prompt = \"\"\"You are an expert in causal reasoning. Given an observed event, identify the most plausible and direct cause(s) from the options.\n",
        "\n",
        "Rules:\n",
        "1. Select the DIRECT cause(s) of the target event\n",
        "2. Multiple options may be correct if they are equally direct causes\n",
        "3. The cause must happen BEFORE the effect\n",
        "4. Distinguish between direct causes and indirect/background factors\n",
        "\n",
        "Output format: Answer with ONLY the letter(s), separated by commas if multiple.\n",
        "Example outputs: \"A\" or \"A, B\"\"\"\n",
        "\n",
        "    user_prompt = f\"Target Event: {instance.target_event}\\n\\n\"\n",
        "    \n",
        "    # 添加上下文\n",
        "    if use_context and instance.docs:\n",
        "        context_parts = []\n",
        "        total_len = 0\n",
        "        for doc in instance.docs[:5]:\n",
        "            title = doc.get(\"title\", \"Document\")\n",
        "            content = doc.get(\"content\", doc.get(\"summary\", \"\"))[:600]\n",
        "            doc_text = f\"[{title}]\\n{content}\"\n",
        "            if total_len + len(doc_text) > max_context_len:\n",
        "                break\n",
        "            context_parts.append(doc_text)\n",
        "            total_len += len(doc_text)\n",
        "        \n",
        "        if context_parts:\n",
        "            user_prompt += \"Context Documents:\\n\" + \"\\n\\n\".join(context_parts) + \"\\n\\n\"\n",
        "    \n",
        "    user_prompt += \"Options:\\n\"\n",
        "    for opt, text in instance.options.items():\n",
        "        user_prompt += f\"{opt}. {text}\\n\"\n",
        "    \n",
        "    user_prompt += \"\\nAnswer:\"\n",
        "    \n",
        "    return system_prompt, user_prompt"
      ],
      "metadata": {
        "id": "prompt_template"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Baseline 方法\n",
        "\n",
        "### 方法 1: OpenAI API"
      ],
      "metadata": {
        "id": "baseline_openai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置 API Key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"  # 替换为你的API Key"
      ],
      "metadata": {
        "id": "set_api_key"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_openai_baseline(data: List[AERInstance], \n",
        "                        model_name: str = \"gpt-4o-mini\",\n",
        "                        use_context: bool = True,\n",
        "                        max_samples: int = None) -> dict:\n",
        "    \"\"\"运行 OpenAI baseline\"\"\"\n",
        "    \n",
        "    client = OpenAI()\n",
        "    \n",
        "    if max_samples:\n",
        "        data = data[:max_samples]\n",
        "    \n",
        "    predictions = []\n",
        "    goldens = []\n",
        "    \n",
        "    for instance in tqdm(data, desc=f\"Running {model_name}\"):\n",
        "        system_prompt, user_prompt = create_prompt(instance, use_context)\n",
        "        \n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                temperature=0,\n",
        "                max_tokens=50\n",
        "            )\n",
        "            pred = parse_prediction(response.choices[0].message.content)\n",
        "        except Exception as e:\n",
        "            print(f\"Error on {instance.id}: {e}\")\n",
        "            pred = set()\n",
        "        \n",
        "        predictions.append(pred)\n",
        "        goldens.append(set(instance.golden_answer) if instance.golden_answer else set())\n",
        "    \n",
        "    return evaluate(predictions, goldens)\n",
        "\n",
        "# 运行示例 (取消注释来运行)\n",
        "# results = run_openai_baseline(dev_data, model_name=\"gpt-4o-mini\", max_samples=10)\n",
        "# print(results)"
      ],
      "metadata": {
        "id": "openai_baseline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方法 2: HuggingFace 本地模型 (推荐用于 Colab)"
      ],
      "metadata": {
        "id": "baseline_hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装额外依赖\n",
        "!pip install -q accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "install_hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "def run_hf_baseline(data: List[AERInstance],\n",
        "                    model_name: str = \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "                    use_context: bool = True,\n",
        "                    max_samples: int = None) -> dict:\n",
        "    \"\"\"运行 HuggingFace 本地模型 baseline\"\"\"\n",
        "    \n",
        "    print(f\"加载模型: {model_name}...\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        load_in_4bit=True  # 4-bit量化以节省显存\n",
        "    )\n",
        "    \n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=50\n",
        "    )\n",
        "    \n",
        "    if max_samples:\n",
        "        data = data[:max_samples]\n",
        "    \n",
        "    predictions = []\n",
        "    goldens = []\n",
        "    \n",
        "    for instance in tqdm(data, desc=f\"Running {model_name.split('/')[-1]}\"):\n",
        "        system_prompt, user_prompt = create_prompt(instance, use_context)\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "        \n",
        "        try:\n",
        "            output = pipe(messages, do_sample=False)\n",
        "            response = output[0][\"generated_text\"][-1][\"content\"]\n",
        "            pred = parse_prediction(response)\n",
        "        except Exception as e:\n",
        "            print(f\"Error on {instance.id}: {e}\")\n",
        "            pred = set()\n",
        "        \n",
        "        predictions.append(pred)\n",
        "        goldens.append(set(instance.golden_answer) if instance.golden_answer else set())\n",
        "    \n",
        "    return evaluate(predictions, goldens)\n",
        "\n",
        "# 运行示例\n",
        "# 推荐模型 (按显存需求从低到高):\n",
        "# - Qwen/Qwen2.5-1.5B-Instruct (约3GB)\n",
        "# - Qwen/Qwen2.5-7B-Instruct (约5GB with 4-bit)\n",
        "# - meta-llama/Llama-3.1-8B-Instruct (约6GB with 4-bit)\n",
        "\n",
        "# results = run_hf_baseline(dev_data, model_name=\"Qwen/Qwen2.5-7B-Instruct\", max_samples=10)\n",
        "# print(results)"
      ],
      "metadata": {
        "id": "hf_baseline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方法 3: 简单 Random Baseline"
      ],
      "metadata": {
        "id": "baseline_random"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def run_random_baseline(data: List[AERInstance], seed: int = 42) -> dict:\n",
        "    \"\"\"随机基线\"\"\"\n",
        "    random.seed(seed)\n",
        "    \n",
        "    predictions = []\n",
        "    goldens = []\n",
        "    \n",
        "    for instance in data:\n",
        "        # 随机选择1-2个选项\n",
        "        n_choices = random.randint(1, 2)\n",
        "        pred = set(random.sample([\"A\", \"B\", \"C\", \"D\"], n_choices))\n",
        "        \n",
        "        predictions.append(pred)\n",
        "        goldens.append(set(instance.golden_answer) if instance.golden_answer else set())\n",
        "    \n",
        "    return evaluate(predictions, goldens)\n",
        "\n",
        "# 运行随机基线\n",
        "random_results = run_random_baseline(dev_data)\n",
        "print(\"Random Baseline 结果:\")\n",
        "print(f\"  Score: {random_results['score']:.4f}\")\n",
        "print(f\"  Exact Match: {random_results['exact_match']:.4f}\")"
      ],
      "metadata": {
        "id": "random_baseline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 结果对比和分析"
      ],
      "metadata": {
        "id": "analysis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 汇总所有结果\n",
        "all_results = {\n",
        "    \"Random\": random_results,\n",
        "    # 取消注释以添加其他结果:\n",
        "    # \"GPT-4o-mini\": gpt_results,\n",
        "    # \"Qwen2.5-7B\": qwen_results,\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(all_results).T\n",
        "df = df.round(4)\n",
        "print(\"\\n=== 结果对比 ===\")\n",
        "print(df.to_string())"
      ],
      "metadata": {
        "id": "compare_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 错误分析"
      ],
      "metadata": {
        "id": "error_analysis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_errors(data: List[AERInstance], \n",
        "                   predictions: List[Set[str]], \n",
        "                   goldens: List[Set[str]],\n",
        "                   n_examples: int = 5):\n",
        "    \"\"\"分析错误案例\"\"\"\n",
        "    \n",
        "    errors = []\n",
        "    for i, (inst, pred, gold) in enumerate(zip(data, predictions, goldens)):\n",
        "        score = calculate_score(pred, gold)\n",
        "        if score < 1.0:\n",
        "            errors.append({\n",
        "                \"instance\": inst,\n",
        "                \"prediction\": pred,\n",
        "                \"golden\": gold,\n",
        "                \"score\": score\n",
        "            })\n",
        "    \n",
        "    print(f\"\\n总错误数: {len(errors)}/{len(data)} ({len(errors)/len(data)*100:.1f}%)\")\n",
        "    print(f\"\\n=== 错误案例分析 (前{n_examples}个) ===\")\n",
        "    \n",
        "    for i, error in enumerate(errors[:n_examples]):\n",
        "        inst = error[\"instance\"]\n",
        "        print(f\"\\n--- 案例 {i+1} ---\")\n",
        "        print(f\"目标事件: {inst.target_event}\")\n",
        "        print(f\"预测: {error['prediction']} | 正确: {error['golden']}\")\n",
        "        print(f\"选项:\")\n",
        "        for opt in error['golden']:\n",
        "            print(f\"  [正确] {opt}: {inst.options[opt][:80]}...\")\n",
        "        for opt in error['prediction'] - error['golden']:\n",
        "            print(f\"  [错误] {opt}: {inst.options[opt][:80]}...\")\n",
        "\n",
        "# 使用示例 (需要先运行某个baseline获得predictions)\n",
        "# analyze_errors(dev_data, predictions, goldens)"
      ],
      "metadata": {
        "id": "error_analysis_code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
