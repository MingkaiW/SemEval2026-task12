# Hyperbolic Embedding集成方法综述

## 1. 替换或增强Transformer（如DeBERTa）Embedding层为双曲空间嵌入
- 将原有的词向量embedding层替换为双曲嵌入（如Poincaré embedding、Hyperbolic NN）。
- 训练时需用双曲空间的距离（如Poincaré distance）和优化器（如Riemannian SGD）。
- 相关论文：Nickel & Kiela, 2017《Poincaré Embeddings for Learning Hierarchical Representations》。
- 代码参考：GitHub搜索“hyperbolic embedding pytorch”，如facebookresearch/poincare-embeddings。
- 适合有明显层级结构或复杂关系的文本、知识图谱任务。
- 能提升模型对层级/树状结构的表达能力，理论上对知识密集型任务提升显著。

## 2. 在知识图谱表示或推理阶段引入双曲几何
- 将KG实体和关系嵌入到双曲空间（如Poincaré ball、Lorentz模型）。
- 下游任务（如KG推理、实体对齐）直接在双曲空间进行。
- 相关论文：Chami et al., 2020《Low-Dimensional Hyperbolic Knowledge Graph Embeddings》。
- 代码参考：GitHub“hyperbolic knowledge graph embedding”，如facebookresearch/hyperbolic-kge。
- 适合KG为主的推理、关系抽取、实体分类等任务。
- 对KG结构复杂、层级明显时提升最大，能更好捕捉实体间的多级关系。

## 3. 融合双曲嵌入与Transformer输出
- Transformer输出与双曲嵌入（如KG或外部知识）进行融合（拼接、加权、注意力等）。
- 可采用多模态融合、特征拼接或设计专门的融合层。
- 相关论文：Chami et al., 2021《Hyperbolic Graph Attention Networks》、部分多模态/知识增强Transformer论文。
- 代码参考：GitHub“hyperbolic fusion transformer”，如相关GAT/Transformer变体。
- 适合需要同时利用文本和知识结构的复杂任务。
- 融合方式灵活，理论上能兼顾Transformer的表达力和双曲空间的结构优势，提升空间较大，但实现复杂度也高。

## 哪种提升更显著？
- 仅文本任务：方法1（直接替换embedding）提升有限，除非任务本身有层级结构。
- 知识图谱为主：方法2提升最显著，尤其在KG结构复杂时。
- 文本+知识融合任务：方法3理论提升最大，能充分利用两种空间的优势，但需精心设计融合方式和调参。

如需具体论文、代码仓库或实现细节，可指定某一方法，可进一步检索和整理。